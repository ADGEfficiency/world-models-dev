##  Setup

Dependencies for this project are
- tensorflow 2.0
- gym

[Resources in rl-resources/world-models](https://github.com/ADGEfficiency/rl-resources/tree/master/world-models)

```bash
git clone git@github.com:ADGEfficiency/mono.git
```

# Methods

## Training from scratch

## Vision

## Sample using a random policy

A dataset is generated by from the environment using a random policy - data is placed into `$HOME/world-models-experiments/random-rollouts`.  The original paper uses 10,000 total episodes, with a max episode length of 1,000.  The dataset generation is parallelzied using Python's `multiprocessing`.

To run a few small episodes as a test (note that each episode will open a new window - this is needed to run the env):

```python
export OBJC_DISABLE_INITIALIZE_FORK_SAFETY=YES
python3 world_models/dataset/generate_dataset.py --num_process 2 --total_episodes 10
```

To run the dataset generation on a remote server (tested on Ubuntu 18.04.2 -  c5.4xlarge 512 GB)

```bash
bash gym-setup.sh

xvfb-run -a -s "-screen 0 1400x900x24 +extension RANDR" -- python3 worldmodels/dataset/sample_random_policy.py --num_process 8 --total_episodes 10000

```

Episode results are saved into individual `tf.record` files, containing the observations and actions for that episode:

```bash TODO
```	

```bash
aws s3 sync random-rollouts/ s3://world-models/random-rollouts
```

## Training the Variational Auto-Encoder (VAE)

Original paper uses 1 epoch, the code based supplied uses 10.

The autoencoder saves a copy of the model into `~/world-models-experiments/vae-training/models`

```bash
export AWS_LOG_LEVEL=3
```

```python
python vision/train_vae.py
```

## Sampling latent stats

Use trained autoencoder

```bash
aws s3 sync s3://world-models/vae-training/models ~/world-models-experiments/vae-training/models

python3 worldmodels/dataset/sample_latent_stats.py --episode_start 0 --episodes 2500

aws s3 sync ~/world-models-experiments/latent-stats  s3://world-models/latent-stats
```

## Training the memory

```bash
python worldmodels/memory/train_memory.py

aws s3 sync ~/world-models-experiments/memory-training  s3://world-models/memory-training
```

## Control

```bash
aws s3 sync s3://world-models/memory-training/models/ ~/world-models-experiments/memory-training/models

aws s3 sync s3://world-models/vae-training/models/ ~/world-models-experiments/vae-training/models

xvfb-run -a -s "-screen 0 1400x900x24 +extension RANDR" -- python3 worldmodels/control/main.py

aws s3 sync control/ s3://world-models/control
```

